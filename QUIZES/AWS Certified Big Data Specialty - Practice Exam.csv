Question,Question Type (multiple-choice or multi-select),Answer Option 1,Answer Option 2,Answer Option 3,Answer Option 4,Answer Option 5,Answer Option 6,Correct Responses,Explanation,Knowledge Area
"<p>You need to load several hundred GB of data every day from Amazon S3 into Amazon Redshift, which is stored in a single file. You’ve found that loading this data is prohibitively slow. </p><p>Which approach would optimize this loading best?</p>",multiple-choice,"<p>Split the data into files between 1MB and 125MB (after compression,) and specify GZIP compression from multiple COPY commands run concurrently.</p>","<p>Split the data into files between 1MB and 125MB (after compression,) and specify GZIP compression from a single COPY command.</p>",<p>Avoid loading the data in the same order as your sort key.</p>,"<p>Split the data into files between 1GB and 125GB (after compression,) and specify GZIP compression from multiple COPY commands run concurrently.</p>",,,2,"Multiple concurrent COPY commands might sound like a good idea, but in reality it forces Redshift to perform a slow, serialized load followed by a VACUUM process. If you want to load data in parallel, it’s best to split your data into separate files no more than 1GB apiece. Compressing the data also helps. Loading data in the same order as your sort key can also speed up the import process, which is the opposite of “c”.",
"<p>Your company has data from a variety of sources, including Microsoft Excel spreadsheets stored in S3, log data stored in a S3 data lake, and structured data stored in Redshift. </p><p>Which is the simplest solution for providing interactive dashboards that span this data?</p>",multiple-choice,"<p>Create an Excel macro to dump the Excel data to csv files stored in S3, then use Amazon Quicksight to visualize it and the other data sources together.</p>","<p>Use Amazon Quicksight directly on top of the Excel, S3, and Redshift data.</p>",<p>Write Python code in a Jupyter Notebook on an EMR cluster that processes the data and generates graphs.</p>,"<p>Create Excel macros to import the log and Redshift data into Excel, and visualize it all within Excel.</p>",,,2,"The key to this question is remembering that Quicksight can actually consume Excel spreadsheets directly. Knowing that, (b) is clearly the simplest approach.",
<p>You are required to maintain a real-time replica of your Amazon Redshift data warehouse across multiple availability zones. </p><p>What is one approach toward accomplishing this?</p>,multiple-choice,"<p>Snapshot your Redshift data to Amazon S3, where it may be restored from any availability zone.</p>","<p>Spin up separate redshift clusters in multiple availability zones, using Amazon Kinesis to simultaneously write data into each cluster. Use Route 53 to direct your analytics tools to the nearest cluster when querying your data.</p>",<p>Enable the multi-AZ option when creating your Redshift cluster.</p>,"<p>Snapshot your Redshift data to Amazon S3, and copy it to a different availability zone periodically from a Lambda function.</p>",,,2,"Redshift does not come with multi-AZ redundancy, so this is something you must build yourself as described in “b”. As S3 is mutli-AZ, snapshotting your data to S3, which Redshift does do automatically, does provide you with some protection - but it does not satisfy the real-time replication requirement stated in the question.",
"<p>You wish to analyze an S3 data lake using standard SQL. Which solution results in the least amount of ongoing administration from you, as your data grows?</p>",multiple-choice,<p>Amazon Redshift Spectrum</p>,<p>Apache Spark on EMR</p>,<p>Amazon RDS</p>,<p>Amazon Athena</p>,,,4,"Of the above choices, only Amazon Athena is a “serverless” solution that does not require you to provision capacity as your data and usage grows.",
<p>A medical company wants to train a machine learning model to classify biopsy results as benign or malignant using a training data set 200GB in size. </p><p>What are TWO appropriate technologies to use for this purpose?</p>,multi-select,<p>Amazon Machine Learning with a binary classification</p>,<p>Amazon Redshift Spectrum</p>,<p>Apache Spark on EMR with MLLib</p>,<p>Amazon SageMaker</p>,<p>Jupyter Notebook on an EC2 instance</p>,,"3,4","The key here is knowing the scalability limits of each solution. Amazon Machine Learning is limited to 100GB of data, and a notebook on an EC2 instance is limited by the power of that single machine. Both Spark and Sagemaker however can scale without limit.",
"<p>An e-commerce company wishes to assign product categories, such as sporting goods or books, to new products that have no category assigned to them. The company has a large corpus of existing product data with manually assigned categories in place. They wish to use their existing data to predict categories on new products, based on other attributes of the products such as its keywords and seller ID, using Amazon Machine Learning. </p><p>Which type of machine learning model would they use?</p>",multiple-choice,<p>Unary classification model</p>,<p>Multi-class classification model</p>,<p>Binary classification model</p>,<p>Regression model</p>,,,2,"Since you are trying to predict categorical data and there are more than two categories, this is a multi-class classification model. Unary classification isn’t really a thing. Under the hood, Amazon ML would use a logistic regression to implement a classification model.",
"<p>A real-estate listing company wishes to predict the sale price of a house, using historical sales data of similar houses based on attributes such as number of bedrooms, zip code, and square footage. </p><p>Which machine learning model in Amazon Machine Learning would be appropriate for this task?</p>",multiple-choice,<p>Unary classification model</p>,<p>Multi-class classification model</p>,<p>Binary classification model</p>,<p>Regression model</p>,,,4,"As we are trying to predict a number (price of a house) and not a category, this is a regression model and not a classification model.",
"<p>A manager wishes to make a case for hiring more people in her department, by showing that the number of incoming tasks for her department have grown at a faster rate than other departments over the past year. </p><p>Which type of graph in Amazon Quicksight would be best suited to illustrate this data?</p>",multiple-choice,<p>Area line chart</p>,<p>Heat map</p>,<p>Stacked vertical bar chart</p>,<p>A sequence of slides in a QuickSight Story</p>,,,1,"When you’re looking for trends over time, line charts are usually the right choice.",
"<p>A produce export company has multi-dimensional data for all of its shipments, such as the date, price, category, and destination of every shipment. A data analyst wishes to interactively explore this data, applying statistical functions to different rows and columns and sorting them in different ways. </p><p>Which QuickSight visualization would be best suited for this?</p>",multiple-choice,<p>Stacked horizontal bar chart</p>,<p>Scatter plot</p>,<p>Pivot table</p>,<p>Heat map</p>,,,3,"Interactive exploration of multi-dimensional data usually calls for a pivot table, as long as you’re not looking to quickly identify outliers at the same time.",
"<p>A produce export company has multi-dimensional data for all of its shipments, such as the date, price, category, and destination of every shipment. A data analyst wishes to explore this data, with the primary purpose of looking for trends and outliers in the information. </p><p>Which QuickSight visualization would be best suited for this?</p>",multiple-choice,<p>Stacked horizontal bar chart</p>,<p>Scatter plot</p>,<p>Pivot table</p>,<p>Heat map</p>,,,4,You can think of heat maps as pivot tables that highlight outliers and trends using color.,
"<p>A large news website needs to produce personalized recommendations for articles to its readers, by training a machine learning model on a daily basis using historical click data. The influx of this data is fairly constant, except during major elections when traffic to the site spikes considerably. </p><p>Which system would provide the most cost-effective and reliable solution?</p>",multiple-choice,"<p>Publish click data into Amazon S3 using Kinesis Streams, and process the data in real time using Splunk on an EMR cluster with spot instances added as needed. Publish the model’s results to DynamoDB for producing recommendations in real-time.</p>","<p>Publish click data into Amazon S3 using Kinesis Firehose, and process the data nightly using Apache Spark and MLLib using reserved instances in an EMR cluster. Publish the model’s results to DynamoDB for producing recommendations in real-time.</p>","<p>Publish click data into Amazon S3 using Kinesis Firehose, and process the data nightly using Apache Spark and MLLib using spot instances in an EMR cluster. Publish the model’s results to DynamoDB for producing recommendations in real-time.</p>","<p>Publish click data into Amazon Elasticsearch using Kinesis Firehose, and query the Elasticsearch data to produce recommendations in real-time.</p>",,,3,"The use of spot instances in response to anticipated surges in usage is the most cost-effective approach for scaling up an EMR cluster, which rules out (b). Kinesis streams is over-engineering because we do not have a real-time streaming requirement, ruling out (a). (d) doesn't make sense because Elasticsearch is not a recommender engine.",
"<p>An organization has a large body of web server logs stored on Amazon S3, and wishes to quickly analyze their data using Amazon Athena. Most queries are operational in nature, and are limited to a single day’s logs. </p><p>How should the log data be prepared to provide the most performant queries in Athena, and to minimize costs?</p>",multiple-choice,"<p>Compress the log data with gzip, and rotate their directories daily</p>","<p>Convert the data into AVRO format, with filenames that may be sorted by date</p>","<p>Convert the data into Apache Parquet format, compressed with Snappy, stored in a directory structure of year=XXXX/month=XX/day=XX/</p>","<p>Convert the data into Apache ORC format, uncompressed, and rotate their directories daily</p>",,,3,"Athena works best with columnar data; Parquet and ORC are both examples of columnar formats (Gzip and AVRO are not.) Compressing the data will allow them to be transferred more efficiently, and partitioning it using the “partition key=value” format in directory names further allows Athena to process only the data it needs.",
"<p>A financial services company has a large, secure data lake stored in Amazon S3. They wish to analyze this data using a variety of tools, including Apache Hive, Amazon Athena, Amazon Redshift, and Amazon QuickSight. </p><p>How should they connect their data and analysis tools in a way that minimizes costs and development work?</p>",multiple-choice,"<p>Write an Apache Spark script on EMR to transform the S3 data into a Hive database on a nightly basis, connect Redshift to the Hive repository, and connect Athena and QuickSight to Redshift.</p>","<p>Run an AWS Glue Crawler on the data lake to populate a AWS Glue Data Catalog. Share the glue data catalog as a metadata repository between Athena, Redshift, Hive, and QuickSight.</p>","<p>Use the AWS Database Migration Service to load the S3 data into an Amazon Aurora database, and use replication to keep the data in sync. Connect Hive, Athena, Redshift, and QuickSight to the Aurora database.</p>","<p>Write an Apache Spark script on EMR to transform and load the S3 data nightly into Amazon Redshift. Connect Hive, Athena, and Quicksight to the Redshift cluster.</p>",,,2,"AWS Glue crops up a lot on the exam. Knowing that Glue can be used to connect to Athena, Redshift, and Quicksight - as well as being used as a Hive metastore - is key to understanding this question. The other options are unnecessarily complex and require development work that could be avoided with Glue.",
"<p>A company wishes to copy 500GB of data from their Amazon Redshift cluster into an Amazon RDS PostgreSQL database, in order to have both columnar and row-based data stores available. The Redshift cluster will continue to receive large amounts of new data every day that must be kept in sync with the RDS database. </p><p>What strategy would be most efficient?</p>",multiple-choice,"<p>Use a materialized view to cache data between the databases, refreshing it periodically using Amazon Lambda</p>",<p>Copy data using the dblink function into PostgreSQL tables</p>,<p>Use the Amazon Database Migration Service to sync the two databases using replication</p>,<p>Use AWS Glue to allow both databases to share a common metadata store</p>,,,2,"Dblink allows you to offload queries in Redshift to another database entirely. A materialized view could work for this, but it will always copy from the beginning of the table, and not just handle what has changed since the last incremental update. Read through https://amzn.to/2fraaHv if this question confused you.",
<p>A financial services company wishes to back up its encrypted data warehouse in Amazon Redshift daily to a different region. </p><p>What is the simplest solution that preserves encryption in transit and at rest?</p>,multiple-choice,"<p>Configure Redshift to automatically copy snapshots to another region, using an AWS KMS customer master key in the destination region.</p>","<p>Configure a second Redshift cluster in another region, and use a daily COPY command triggered by Lambda to transfer the encrypted data to the other cluster.</p>",<p>Configure Redshift to replicate its data to a standby cluster in another region.</p>,<p>Move the data into S3 and use Amazon Redshift from multiple regions to query it.</p>,,,1,"Note that we said “daily,” which means we don’t need real-time replication. Snapshots will do just fine. “D” is a little bit misleading, as it doesn’t matter where you run Redshift itself from if the data is stored elsewhere - but moving the data isn’t exactly a simple solution.",
<p>An Amazon Elasticsearch domain has been installed within a VPC. </p><p>What are TWO methods which could be employed to securely allow access to Kibana from outside the VPC?</p>,multi-select,<p>Set up an SSH tunnel with port forwarding to allow access on port 5601.</p>,<p>Set up a reverse proxy server between your browser and Amazon Elasticsearch Service.</p>,<p>Use KMS between your browser and Kibana.</p>,<p>Create a security group to allow your IP access to the subnet attached to the VPC on port 443.</p>,<p>Configure an IAM policy to allow your browser to access Kibana.</p>,,"1,2","Kibana runs on port 5601 by default, so opening up port 443 won’t help. IAM doesn’t know about your browser, and KMS does not integrate with Kibana. Only a and b are viable solutions.",
<p>New data arrives in S3 on an irregular schedule that you wish to import into an Amazon Elasticsearch cluster as it is received. The raw data in S3 requires some custom parsing before it is loaded into Elasticsearch. </p><p>Which solution minimizes ongoing maintenance and maximizes scalability?</p>,multiple-choice,<p>Use AWS Glue to connect Elasticsearch directly to your S3 bucket</p>,"<p>Use a Lambda function to respond to event triggers from S3, and stream data from S3 into Elasticsearch as it is received</p>",<p>Use Spark Streaming on an EMR cluster with reserved instances to monitor the S3 bucket and redirect the data to Elasticsearch</p>,<p>Use Kinesis Firehose to pump data from S3 directly into Elasticsearch</p>,,,2,"Glue does not integrate with Elasticsearch, ruling out (a). EMR clusters require manual scaling as your needs evolve, which involves ongoing maintenance we could avoid - so c is also a poor choice. While Firehose can sit between S3 and Elasticsearch, the requirement to perform custom parsing means a Lambda function would be useful - leading us to (b).",
"<p>A review website wishes to perform sentiment analysis on written reviews, using a large-scale recurrent neural network. </p><p>Which TWO EC2 instance types might be appropriate choices for this application?</p>",multi-select,<p>P3</p>,<p>M5</p>,<p>R5</p>,<p>C5</p>,<p>G3</p>,,"1,5","P3 and G3 are both GPU instance types. Deep learning applications such as this benefit from multiple GPU’s, and can scale sub-linearly on these instance types.",
<p>A hospital monitoring sensor data from heart monitors wishes to raise immediate alarms if an anomaly in any individual’s heart rate is detected. </p><p>Which architecture meets these requirements in a scalable manner?</p>,multiple-choice,"<p>Publish sensor data into S3, and use Kinesis Firehose to publish the data into a Spark Streaming application that detects anomalies and raises alarms.</p>","<p>Publish sensor data into S3, and use AWS Glue to query the data using Amazon Redshift Spectrum. Run a periodic script to query the data for anomalies using SQL and raise alarms when needed.</p>","<p>Publish sensor data into a Kinesis data stream, and route the data into a custom application on an EC2 instance that analyzes the data for anomalies and sends out alarms as needed.</p>","<p>Publish sensor data into a Kinesis data stream, and create a Kinesis Data Analytics application using RANDOM_CUT_FOREST to detect anomalies. When an anomaly is detected, use a Lambda function to route an alarm to Amazon SNS.</p>",,,4,"RANDOM_CUT_FOREST is a function in Kinesis Data Analytics intended for anomaly detection. By using serverless services such as Kinesis, Lambda, and SNS we ensure the scalability of this system, and the choice of Kinesis Streams instead of Firehose ensures real-time delivery of the data.",
"<p>A MapReduce job on an EMR cluster needs to process data that is currently stored in very large, compressed files in HDFS, which limits the cluster’s ability to distribute its processing. </p><p>Which TWO solutions would best help the MapReduce job to operate more efficiently?</p>",multi-select,<p>Compress the file with GZIP</p>,<p>Convert the file to AVRO format</p>,<p>Compress the file with Snappy</p>,<p>Uncompress the data and split it into 64MB chunks</p>,<p>Uncompress the data and split it into 64K chunks</p>,,"2,4","AVRO is a format that can easily be split and distributed by MapReduce as needed. Alternately, the data could be split manually into different files - but ideally you’d want these files to be close to the default HDFS chunk size of 64MB in order to make the best use of HDFS.",
<p>You are tasked with using Hive on Elastic MapReduce to analyze data that is currently stored in a large relational database. </p><p>Which approach could meet this requirement?</p>,multiple-choice,<p>Use Apache Sqoop on the EMR cluster to copy the data into HDFS</p>,<p>Use Apache Flume to stream data from the database into HDFS</p>,<p>Use AWS Glue to create a Hive metastore used by your EMR cluster</p>,"<p>Extract the database into csv files in Amazon S3, and point your EMR file system to that S3 bucket</p>",,,1,"Sqoop is an open-source system for transferring data between Hadoop and relational databases. Flume is intended for real-time streaming applications, Glue is intended for use with S3, and you can’t direct EMRFS to arbitrary S3 buckets.",
"<p>A data scientist wishes to develop a machine learning model to predict stock prices using Python in a Jupyter Notebook, and use a cluster on AWS to train and tune this model, and to vend predictions from it at large scale. </p><p>Which system allows you to do this?</p>",multiple-choice,<p>Apache Zeppelin</p>,<p>Amazon SageMaker</p>,<p>Amazon Machine Learning</p>,<p>Amazon Comprehend</p>,,,2,"SageMaker enables developers and data scientists to build, train, and deploy machine learning models at any scale, using hosted Jupyter notebooks. Zeppelin also uses a hosted Jupyter notebook, but does not integrate with AWS to train and tune models. Amazon Machine Learning does not use notebooks, and Amazon Comprehend is a natural language processing tool.",
<p>A real estate company wishes to display interactive charts on their website summarizing their prior month’s sales activity. </p><p>Which TWO solutions would provide this capability in a scalable and inexpensive manner?</p>,multi-select,"<p>Publish data in csv format to Amazon Cloudfront via S3, and use d3.js to visualize the data on the web.</p>","<p>Embed Amazon Quicksight into the website, querying an underlying Redshift data warehouse.</p>",<p>Embed a Jupyter Notebook hosted on EMR into the website.</p>,"<p>Use Tableau to visualize the data on the web, backed by an Amazon Aurora RDS database.</p>","<p>Publish data in csv format to Amazon Cloudfront via S3, and use Highcharts to visualize the data on the web.</p>",,"1,5",Both d3 and Highcharts are Javascript libraries intended for interactive charts and graphs on websites. Quicksight and Tableau are data analysis tools not intended for public deployment.,
"<p>An application processes sensor data in real-time by publishing it to Kinesis Data Streams, which in turn sends data to an AWS Lambda function that processes it and feeds it to DynamoDB. During peak usage periods, it’s been observed that some data is lost. You’ve determined that you have sufficient capacity allocated for Kinesis shards and DynamoDB reads and writes. </p><p>What might be TWO possible solutions to the problem?</p>",multi-select,<p>Allocate more Lambda nodes</p>,<p>Process data in smaller batches to avoid hitting Lambda’s timeout</p>,<p>Increase your Lambda function’s timeout value</p>,<p>Provision more Kinesis shards</p>,<p>Enable replication in Lambda</p>,,"2,3","Lambda is serverless, and you don’t need to concern yourself with how many nodes it might be using under the hood, or how it automatically handles replication and redundancy. However, lambda functions do have a timeout associated with them, and if the processing you’re doing takes longer than this, events will be rejected.",
<p>You wish to use Amazon Redshift Spectrum to analyze data in an Amazon S3 bucket that is in a different account than Redshift Spectrum. </p><p>How would you authorize access between Spectrum and S3 across accounts?</p>,multiple-choice,<p>Create an IAM role allowing access for COPY operations on S3</p>,<p>Create a VPC endpoint to S3 accessible by your Redshift Spectrum</p>,<p>Add a policy to the S3 bucket allowing S3 GET and LIST operations for an IAM role for Spectrum on the Redshift account</p>,<p>Add a policy to the S3 bucket allowing S3 GET and LIST operations for the user agent “AWS Redshift/Spectrum”</p>,,,3,Only “c” specifically addresses the problem of cross-account access to the S3 bucket.,
<p>What are THREE ways in which EMR integrates Pig with Amazon S3?</p>,multi-select,<p>Loading custom JAR files from S3 with the REGISTER command</p>,<p>Submitting work from the EMR console using Pig scripts stored in S3</p>,<p>Integration with AWS Glue to infer schemas on S3 data</p>,<p>Directly writing to HCatalog tables in S3</p>,<p>Importing permissions from S3 buckets for use in your Pig Latin scripts</p>,,"1,2,4","You can configure EMRFS, backed by S3, as your data store in Pig, which allows it to read and write to S3 as it would with HDFS.",
"<p>You wish to train a large convolutional neural network for classifying street sign images from cameras on self-driving cars, using an EMR cluster. </p><p>What are TWO choices of technologies included in EMR clusters that might help you to build this?</p>",multi-select,<p>Mahout</p>,<p>MXNet</p>,<p>Presto</p>,<p>Splunk</p>,<p>Tensorflow</p>,,"2,5","MXNet and Tensorflow are both libraries used for building neural networks, and contain libraries that are helpful in creating convolutional neural networks.",
"<p>Which tool on Amazon Elastic MapReduce allows you to monitor your cluster’s performance as a whole, and at individual nodes?</p>",multiple-choice,<p>Hue</p>,<p>Presto</p>,<p>Ganglia</p>,<p>Ambari</p>,,,3,"Ganglia is the operational dashboard provided with EMR. Hue and Ambari are graphical front-ends for interacting with a cluster, but only Hue is provided with EMR. Presto is used for querying multiple data stores at once.",
<p>What are THREE ways in which EMR optionally integrates HBase with S3?</p>,multi-select,<p>Snapshots of HBase data to S3</p>,<p>AWS Glue integration to infer schemas of S3 data</p>,<p>Storage of HBase StoreFiles and metadata on S3</p>,<p>Automatic backup from HDFS-based StoreFiles to S3</p>,<p>HBase read-replicas on S3</p>,,"1,3,5","EMR allows you use S3 instead of HDFS for HBase’s data via EMRFS. Although you can export snapshots to S3, HBase itself does not automate this for you.",
"<p>You have created a system that recommends items similar to other items on an e-commerce website, by training a recommender system using Mahout on an EMR cluster. </p><p>Which would be a performant means to vend the resulting table of similar items for any given item to the website at high transaction rates?</p>",multiple-choice,<p>Expose the data via Hive and JDBC</p>,<p>Publish the data into HBase</p>,<p>Load the data into Amazon Aurora / RDS and query it from the website</p>,<p>Load the data into a Redshift cluster and query it from the website</p>,,,2,"This is an OLTP use case for which a “NoSQL” database is a good fit. HBase is the only option presented designed for OLTP and not OLAP, plus it has the advantage of already being present in EMR. DynamoDB would also be an appropriate technology to use.",
"<p>You need to ETL streaming data from web server logs as it is streamed in, for analysis in Athena. Upon talking to the stakeholders, you’ve determined that the ETL does not strictly need to happen in real-time, but transforming the data within a minute is desirable. </p><p>What is a viable solution to this requirement?</p>",multiple-choice,"<p>Create an AWS Glue ETL job, and schedule it to run every minute.</p>","<p>Perform any initial ETL you can using Amazon Kinesis, store the data in S3, and trigger a Glue ETL job to complete the transformations needed.</p>","<p>Publish your data into DynamoDB, and use DynamoDB streams together with AWS Glue to perform the ETL.</p>","<p>Perform any initial ETL you can with Kinesis Analytics, and send the output of Kinesis Analytics directly to a Spark Streaming application to handle the remaining transformations.</p>",,,2,"Glue jobs can be scheduled at a minimum of 5 minutes, ruling out “a”. Glue does not integrate directly with DynamoDB, ruling out “c”. Kinesis Analytics cannot connect directly to Spark Streaming, ruling out “d”. “B” is the recommended approach from AWS. An alternative approach would be to use Kinesis Firehose and a Lambda function to transform the data before storing it in S3.",
"<p>Your company wishes to monitor social media, and perform sentiment analysis on Tweets to classify them as positive or negative sentiment. You are able to obtain a data set of past Tweets about your company to use as training data for a machine learning system, but they are not classified as positive or negative.</p><p>How would you build such a system?</p>",multiple-choice,"<p>Use Amazon Mechanical Turk to label past Tweets as positive or negative, and use those labels to train a neural network on an EMR cluster.</p>","<p>Use Amazon Machine Learning with a binary classifier to assign positive or negative sentiments to the past Tweets, and use those labels to train a neural network on an EMR cluster.</p>","<p>Stream both old and new tweets into an Amazon Elasticsearch Service cluster, and use Elasticsearch machine learning to classify the tweets.</p>",<p>Use RANDOM_CUT_FOREST to automatically identify negative tweets as outliers.</p>,,,1,"A machine learning system needs labeled data to train itself with; there’s no getting around that. Only option “a” produces the positive or negative labels we need, by using humans to create that training data initially. Another solution would be to use natural language processing through a service such as Amazon Comprehend, but we don’t expect you to see Comprehend on the exam yet.",
"<p>Your team has developed a Spark Streaming applications that performs real time transformations on an on-premise Apache Kafka cluster and finally delivers the data in real time to S3. As part of a migration to the cloud and switch to Kinesis as an underlying streaming store, what do you recommend?</p>",multiple-choice,"<p>Produce data using Spark Streaming to Kinesis Data Streams, and read the data with Spark Streaming from Kinesis Data Firehose to write it to S3.</p>","<p>Produce data using Spark Streaming to Kinesis Data Streams, and read the data with Spark Streaming from Kinesis Data Streams to write it to S3.</p>",<p>Produce data using Spark Streaming to Kinesis Data Firehose and deliver the data to S3 using Kinesis Data Firehose.</p>,<p>Produce data using Spark Streaming to Kinesis Data Firehose and deliver the data to S3 using Spark Streaming reading from Kinesis Data Streams.</p>,,,2,"Here, the key requirement is the real-time constraint. Kinesis Data Firehose is “near real time” (60 seconds minimum batch size) and thus cannot be used for delivery to S3 in real time. Finally, Spark Streaming can read and write to Kinesis Data Streams only.",
"<p>Your financial organization has hundreds of Terabytes of data stored within its on premise data centers, and data is being produced at the rate of Gigabytes per second, and could be consumed within 3 days. As part of their AWS cloud migration, what solution do you recommend for them?</p>",multiple-choice,<p>Install Direct Connect between your data center and AWS and transfer all the data over a 10Gbps line. Use Kinesis Data Streams for ongoing data collection.</p>,<p>Install Direct Connect between your data center and AWS and transfer all the data over a 10Gbps line. Use Kinesis Data Firehose for ongoing data collection.</p>,<p>Transfer your historical data using Snowball and use Kinesis Data Firehose for ongoing data collection.</p>,<p>Transfer your historical data using Snowball and use Kinesis Data Streams for ongoing data collection.</p>,,,4,"As your organization has hundreds of Terabytes of data, it is going to be much quicker and much more efficient to use Snowball to transport that data to AWS. As the data streams must be stored for at least 3 days, you must use Kinesis Data Streams which has a configurable data retention of between 1 and 7 days.",
<p>You are dealing with PII datasets and would like to leverage Kinesis Data Streams for your pub-sub solution. Regulators imposed the constraint that the data must be encrypted end-to-end using an internal key management system. What do you recommend?</p>,multiple-choice,<p>Enable KMS encryption at rest for Kinesis and transfer the data over HTTPS to get in-flight encryption</p>,<p>Encrypt the data with the MD5 algorithm before pushing it to Kinesis</p>,<p>Use the Kinesis Client Library (KCL) to encrypt the data</p>,<p>Implement a custom encryption code in the Kinesis Producer Library (KPL)</p>,,,4,"We cannot use KMS as we cannot leverage an internal key management system. MD5 is not an encryption algorithm and encryption must happen at the producer level (KPL), while decryption will happen at the consumption level (KCL).",
"<p>As an e-commerce retailer, you would like to onboard clickstream data onto Kinesis from your web servers Java applications. You want to ensure that a retry mechanism is in place, as well as good batching capability and asynchronous mode. You also want to collect server logs with the same constraints. What do you recommend?</p>",multiple-choice,<p>Use the AWS Kinesis SDK for Java in your web servers and use Kinesis Producer Library to collect server logs</p>,<p>Use Kinesis Producer Library to send the click stream and collect the server logs.</p>,<p>Use the Kinesis Producer Library to send the clickstream and the Kinesis agent to collect the Server Logs</p>,<p>Use the Kinesis Agent to send the click stream and collect the server logs.</p>,,,3,"The KPL has the mechanisms in place for retry and batching, as well as asynchronous mode. The Kinesis agent is meant to retrieve server logs with just configuration files.",
"<p>You would like to process data coming from IoT devices, and processing that date takes approximately 2 minutes per data point. You would also like to be able to scale in terms of number of processes that will consume that data, based on the load your are receiving, and no ordering constraints are required. What do you recommend?</p>",multiple-choice,<p>Define an IoT rules actions to send data to Kinesis Data Streams and consume the data with KCL</p>,<p>Define an IoT rules actions to send data to Kinesis Data Streams and consume the data with AWS Lambda</p>,<p>Define an IoT rules actions to send data to SQS and consume the data with EC2 instances in an Auto Scaling group</p>,<p>Define an IoT rules actions to send data to Kinesis Data Firehose and consume with AWS Lambda</p>,,,3,"Here the lack of ordering and the fact the processing may be long, and needs to scale based on the number of messages make SQS a better fit for this than Kinesis Data Streams, that on top of it will be more cost efficient. ",
"<p>You are working for an e-commerce website and that website uses an on-premise PostgreSQL database as its main OLTP engine. You would like to perform analytical queries on it, but the Solutions Architect recommended not doing it off of the main database. What do you recommend?</p>",multiple-choice,<p>Create an RDS Read Replica</p>,<p>Create an on-premise PostgreSQL Read Replica</p>,<p>Create an Aurora Read Replica</p>,<p>Use DMS to replicate the database to RDS</p>,,,4,Read Replicas cannot be instantiated from an on-premise database. Here using a solution like DMS (Database Migration Service) is the right way to replicate the database state. ,
"<p>Your enterprise would like to leverage AWS Redshift to query data. Currently that data is produced at the rate of 5 PB of historical data and another 3 TB per month that you would like to get with less than two days delay, and you are tasked with finding the most efficient data transfer solution into S3. What do you recommend? (select two)</p>",multi-select,<p>Establish Direct Connect and transfer the historical data over a 10Gpbs connection</p>,<p>Establish Site-to-Site VPN and transfer the historical data over a public 10Gbps connection.</p>,<p>Use Snowball to transfer the historical data</p>,<p>Use Snowball to transfer the newly created monthly data</p>,<p>Establish Direct Connect and do a daily upload newly created monthly data directly into S3</p>,,"3,5","Snowball is the only way to transfer your historical data as it would take a very long time to transfer over the network, even with Direct Connect or Site-to-Site VPN. For ongoing data, it’s the equivalent of 300 GB a day, which will take less than an hour to upload each day, versus Snowball which will definitely take longer than 2 days to reach AWS.",
<p>You are working for a bank and your company is regularly uploading 100 MB files to Amazon S3 and analyzed by Athena. It has come to light that recently some of the uploads have been corrupted and made a critical big data job fails. Your company would like a stronger guarantee that uploads are done successfully and that the files have the same content on premise and on S3. It looks to do so at minimal cost. What do you recommend?</p>,multiple-choice,<p>Use the S3 ETag and compare to the local MD5 hash</p>,<p>Download the data from S3 after uploading it and compare the SHA-1 signature</p>,<p>Upload a hash header as part of the S3 metadata which will trigger an S3 failure upon server side check for corruption</p>,<p>Transfer the data over HTTP with gzip compression enabled</p>,,,1,"Here the S3 tag will compute the MD5 hash of the file on the server and this can be used to compare it to the local MD5 hash of the file that has been uploaded. Downloading data back from S3 might work, but will be way too costly. There’s no feature to upload a hash header to S3 and transferring the data on HTTP will not help, and will decrease security. ",
"<p>You are creating an EMR cluster that will process the data in several MapReduce steps. Currently you are working against the data in S3 using EMRFS, but the network costs are extremely high as the processes write back temporary data to S3 before reading it. You are tasked with optimizing the process and bringing the cost down, what should you do?</p>",multiple-choice,<p>Enable LUKS encryption</p>,<p>Add a preliminary step that will use a S3DistCp command</p>,<p>Enable EMRFS local caching feature</p>,<p>Use I3 type of instances</p>,,,2,"Here, using an S3DistCp command is the right thing to do to copy data from S3 into HDFS and then make sure the data is processed locally by the EMR cluster MapReduce job. Upon completion, you will use S3DistCp again to push back the final result data to S3. LUKS encryption will not help, EMRFS does not have a local caching feature, and changing the EC2 instance types won’t help",
<p>You are looking to reduce the latency down from your Big Data processing job that operate in Singapore but source data from Virginia. The Big Data job must always operate against the latest version of the data. What do you recommend?</p>,multiple-choice,<p>Create a Data Pipeline job to transfer the data regularly between two S3 buckets located in different regions</p>,<p>Enable S3 Cross Region Replication</p>,<p>Install a CloudFront distribution on top of your S3 bucket</p>,<p>Use S3 Transfer Acceleration</p>,,,2,"Here to reduce the latency you must move the data to another S3 bucket. The DataPipeline job may work but won’t be able to replicate the latest data continuously, and CloudFront will cache some data and may provide outdated data to the Big Data Job. S3 Transfer acceleration will not help. Here, you must enable S3 Cross Region Replication",
"<p>You have an ETL process that collects data from different sources and 3rd party providers and would like to ensure that data is loaded into Redshift once all the parts from all the providers related to one specific jobs have been gathered, which is the process that can happen over the course of one hour to one day. What the least costly way of doing that?</p>",multiple-choice,<p>Load data in Redshift regularly using a Lambda cron job into a temporary table and use Redshift database triggers to assemble the final data when all the parts are ready.</p>,<p>Create an AWS Lambda that responds to S3 upload events and will check if all the parts are there before uploading to Redshift</p>,<p>Use the multi part upload feature of S3 to collect data from different sources</p>,<p>Create a Kinesis Data Firehose pipeline and program a Python condition that will trigger a buffer flush upon receiving all the necessary parts</p>,,,2,Loading the data in Redshift will be costly and inefficient and Redshift does not support database triggers. Multi Part is not helpful for data coming from various sources. Kinesis Firehose does not have a Python condition for buffer flush (only time and size). Here a Lambda function that reacts to events happening in S3 is the right answer. ,
"<p>Your company collects data from various sources into Kinesis and the stream is delivered using Kinesis Data Firehose into S3. Once in S3, your data scientist team uses Athena to query the most recent data, and usage has shown that after a month, the team queries the data less and less, and finally after two months does not use it. For regulatory reasons, it is required to keep all the data for 5 years and no one should be able to delete it. What do you recommend? (select two)</p>",multi-select,<p>Create a lifecycle rule to migrate the data to S3 IA after 30 days and Glacier after 60 days.</p>,<p>Create a lifecycle rule to migrate the data to S3 Athena one day 1 and S3 Athena IA after 30 days.</p>,<p>Create a lifecycle rule to migrate all the data to S3 IA after 30 days and delete the data after 60 days. Create a rule to have a replica of all source data into Glacier from the first day.</p>,<p>Implement a Glacier Vault Lock Policy</p>,<p>Implement a Glacier Vault Access Policy</p>,,"3,4","S3 Athena is not a storage class. Data should be moved to S3 IA after 30 days. If the data is moved to Glacier after 60 days, then there are no guarantees for it not to be deleted by anyone for the first 60 days. Hence a replica of the data should be delivered to Glacier right away. Finally, to prevent deletion in Glacier, you must use a Glacier Vault Lock Policy. ",
"<p>You are processing data using a long running EMR cluster and you like to ensure that you can recover data in case an entire availability zone goes down, as well as process the data locally for the various Hive jobs you plan on running. What do you recommend to do this at a minimal cost?</p>",multiple-choice,<p>Store the data in S3 and keep a warm copy in HDFS</p>,<p>Store the data in HDFS and mirror it to another EMR cluster in another region</p>,<p>Store the data in S3 and directly perform Hive jobs against it</p>,<p>Enable regular EBS backups</p>,,,1,"EBS backups will be expensive and very painful to deal with as the dataset will be distributed over many different snapshots due to HDFS block replication mechanisms. Storing all the data in S3 and none in HDFS will incur some extra costs as the data is constantly read and written back to S3. It’s better to keep a local copy on HDFS. Finally, maintaining an entirely new EMR cluster for disaster recovery is too expensive. ",
"<p>Your daily Spark jobs runs against files created by a Kinesis Firehose pipeline in S3. Due to a low throughput, you observe that each of the many files created by Kinesis Firehose is about 100KB. You would like to optimise your Spark job as best as possible to query the data efficiently. What do you recommend?</p>",multiple-choice,<p>Consolidate files on a daily basis using DataPipeline</p>,<p>Compress the files using GZIP</p>,<p>Increase the batch flush time in Kinesis Data Firehose to 1 hour.</p>,<p>Use multi-part download with Spark.</p>,,,1,"Multi part download is not a Spark feature, which does not deal well with many small files. GZIP will not solve the problems, the files will still be small and you will have many of them. Kinesis Data Firehose does not have a flush time greater than 5 minutes, which will still create many small files. Bottom line, you should use a DataPipeline job to consolidate the files on a daily basis into a larger file.",
<p>You are collecting data from various IoT thermostat and would like to have the data in S3 to ensure you can perform analytics on it using Hive running on EMR. The data scientists will use Hive to query the data based on a date range and will need to delete the data in S3 regularly if it becomes out of date to save cost. What do you recommend for the storage strategy in S3?</p>,multiple-choice,<p>&lt;device-id&gt;/&lt;YYYY&gt;/&lt;MM&gt;/&lt;DD&gt;</p>,<p>&lt;YYYY&gt;/&lt;MM&gt;/&lt;DD&gt;/&lt;device-id&gt;/</p>,<p>All the data at the root of your S3 bucket</p>,<p>&lt;DD&gt;/&lt;MM&gt;/&lt;YYYY&gt;/&lt;device-id&gt;/</p>,,,2,"As all the data will often get queried and deleted based on a date range, it’s much better to use a date as the root key for your S3 objects. Starting with the year is better as it will allow you to query based on years efficiently. ",
"<p>You work for a gaming company and each game’s data is stored in DynamoDB tables. In order to provide a game search functionality to your users, you need to move that data over to ElasticSearch. How can you achieve it efficiently and as close to real time as possible?</p>",multiple-choice,<p>Enable DynamoDB Streams and connect it with Kinesis Data Firehose</p>,<p>Enable DynamoDB Streams and write a Lambda function</p>,<p>Create a DataPipeline job scheduled on an hourly basis</p>,<p>Use DynamoDB Global Replication and select ElasticSearch Service as your target.</p>,,,2,"DynamoDB Streams do not have direct integration with Firehose, DataPipeline will not have the data fast enough into ElasticSearch, and DynamoDB Global Tables do not integrate with ElasticSearch. Here we need to write a Lambda function that is triggered from a DynamoDB Stream",
<p>You are looking to query data storage in DynamoDB from your EMR cluster. Which of the following technology will allow you to do so?</p>,multiple-choice,<p>Spark</p>,<p>HBase</p>,<p>Pig</p>,<p><strong>Hive</strong></p>,,,4,"Amongst the four listed above, Hive can have DynamoDB as a source for its queries. ",
"<p>You are storing gaming data for your game that is becoming increasingly popular. An average game data will contain 80KB of data and as your games are quick. You expect about 400 games to be written per second to your database. Additionally, a lot of people would like to retrieve this game data and you expect about 1800 eventually consistent reads per second. How should you provision your DynamoDB table?</p>",multiple-choice,<p>32000 WCU &amp; 144000 RCU</p>,<p>32000 WCU &amp; 36000 RCU</p>,<p>3200 WCU &amp; 36000 RCU</p>,<p>32000 WCU &amp; 18000 RCU</p>,,,4,1 WCU = 1 KB / s so we need 80KB * 400 / s = 32000 WCU. 1 RCU = 2 eventually consistent reads per second of 4 KB so we need 1800 * 80 / 8 = 18000 RCU.,
<p>You are an online retailer and your website is a storefront for millions of products. You have recently run a big sale on one specific electronic and you have encountered Provisioned Throughput Exceptions. You would like to ensure you can properly survive an upcoming sale that will be three times as big. What do you recommend?</p>,multiple-choice,<p>Enable DynamoDB Streams</p>,<p>Enable DynamoDB DAX</p>,<p>Triple the provisioned RCU</p>,<p>Triple the provisioned WCU</p>,,,2,"Here we are in a hot partition situation where one item is constantly being requested. Adding RCU will be costly and won’t be effective as even though we have tripled the table capacity, the one partition will still experience throughput exceptions. Enabling DynamoDB Streams will not help, but a DAX cluster will provide caching for hot items and will definitely help in this situation.",
"<p>You are deploying your web servers being a Load Balancer. The Load Balancer is in a public subnet and the web servers sit in a private subnet. For security reasons, the private subnet does not have access to internet and you would like to ensure the web servers within this subnet have access to DynamoDB. What do you recommend?</p>",multiple-choice,<p>Create an Internet Gateway in the public subnet</p>,<p>Enable Direct Connect between your VPC and DynamoDB</p>,<p>Provision a VPC Endpoint Gateway</p>,<p>Create a DynamoDB table within your VPC and attach a security group to it authorizing traffic from your EC2 instances.</p>,,,3,"An internet gateway will work but will break the security in place. Direct Connect only helps connecting on premise data centers to AWS VPC. DynamoDB tables cannot be provisioned within a VPC. So you need to create a VPC Endpoint of type Gateway (please note they’re not Interfaces, they’re gateway). ",
<p>You would like to design an application that will be able to sustain storing 100s of TB of data in a database that will get low latency on reads and won’t require you to manage scaling. What do you recommend?</p>,multiple-choice,<p>Redshift</p>,<p>DynamoDB</p>,<p>Aurora</p>,<p>ElastiCache</p>,,,2,"Redshift cannot be used for low latency reads, Aurora will not scale past 64 TB and ElastiCache most likely won’t scale and require you to manage scaling anyway. DynamoDB complies with the requirements and has built-in support for auto scaling. ",
"<p>As part of your application development, you would like your users to be able to get Row Level Security. The application is to be deployed on web servers and the users of the application should be able to use their amazon.com accounts. What do you recommend for the database and security?</p>",multiple-choice,<p>Enable Web Identity federation. Use DynamoDB and reference ${aws:username} in the attached IAM policy</p>,<p>Enable Web Identity federation. Use DynamoDB and reference ${www.amazon.com:user_id} in the attached IAM policy</p>,<p>Enable Web Identity federation. Use RDS and reference ${aws:username} in the attached IAM policy</p>,<p>Enable Web Identity federation. Use RDS and reference ${www.amazon.com:user_id} in the attached IAM policy</p>,,,2,"RDS does not allow authorization through IAM policies, so we have to rule out RDS as a database technology for our purpose. DynamoDB is the technology of choice. As we’re using Web Identity Federation with amazon.com, our IAM policy should use the ${www.amazon.com:user_id} policy variable",
"<p>As part of an effort to limit cost and maintain under control the size of your DynamoDB table, your ÅWS account manager would like to ensure old data is deleted in DynamoDB after 1 month. How can you do so with as little maintenance as possible and without impacting the current read and write operations?</p>",multiple-choice,<p>Create a lifecycle rule to delete your items after 30 days</p>,<p>Create a Lambda function that gets triggered on a daily basis</p>,<p>Scan your table from EMR and issue a delete statement afterwards</p>,<p>Enable DynamoDB TTL and add a TTL column</p>,,,4,"Lifecycle rules do not exist on DynamoDB, and a Lambda function would require maintenance and add load on the WCU as you delete items. So will an EMR scan. The TTL will automatically expire old items and will not affect your usage of WCU and RCU upon item deletion. ",
<p>What security mechanisms are supported by EMR? (Select three)</p>,multi-select,<p>LUKS encryption</p>,<p>KMS encryption</p>,<p>External HSM encryption</p>,<p>SSE-C</p>,<p>Transparent Data Encryption (TDE)</p>,<p>SSE-KMS</p>,"1,2,6",https://aws.amazon.com/blogs/big-data/best-practices-for-securing-amazon-emr/ ,
"<p>You have an S3 bucket that your entire organization can read. For security reasons you would like the data sits encrypted there and you would like to define a strategy in which users can only read the data which they are allowed to decrypt, which may be a different partial set of objects within the bucket for each user. How can you achieve that?</p>",multiple-choice,<p>Use SSE-KMS to encrypt the files</p>,<p>Use SSE-S3 to encrypt the files</p>,<p>Change the bucket policy</p>,<p>Use identity federation</p>,,,1,"Here, the key is that users should be able to decrypt different files based on their rights. Bucket policies do not scale for that matter if you have a lot of users and diverse data directories. SSE-S3 uses only one encryption key and does not enable granular permissions. Identity federation would not help either to solve that problem, as it does not define an authorization model. SSE-KMS will allow you to use different KMS keys to encrypt the objects, and then you can grant users access to specific sets of KMS keys to give them access to the objects in S3 they should be able to decrypt.",
<p>Which are the two technologies that support VPC Endpoint Gateway? (select two)</p>,multi-select,<p>SQS</p>,<p>DynamoDB</p>,<p>S3</p>,<p>CloudFormation</p>,<p>SNS</p>,,"2,3","DynamoDB & S3 are the only technologies that have VPC Endpoint Gateways, the rest are VPC Endpoint Interfaces. ",
<p>You are launching an EMR cluster and plan on running custom python scripts that will end up invoking custom Lambda functions deployed within your VPC. How can you ensure the EMR cluster has the right to invoke the functions?</p>,multiple-choice,<p>Create an IAM user and put the credentials into the ~/.aws/credentials file</p>,<p>Create an AWS Lambda policy and authorize the EMR cluster security groups</p>,<p>Create an IAM role and attach it to the EMR instances</p>,<p>Attach a VPC Endpoint to Lambda and map a route to EMR</p>,,,3,"As EMR is made of EC2 instances, the best and most secure way for them to be allowed to invoke a Lambda function is to attach IAM roles. There’s no Lambda policy or VPC Endpoint that can help for that matter. ",
<p>A user or application has been changing configuration on your production S3 bucket and you would like to understand who did this. What do you recommend?</p>,multiple-choice,<p>Analyze the S3 access logs with Athena</p>,<p>Use CloudTrail</p>,<p>Use the IAM action logs</p>,<p>Write a custom S3 bucket policy</p>,,,2,"Very simply, we’re looking at searching through API calls done on your account, which CloudTrail was made for.",
<p>Your esports application hosted on AWS need to process game results immediately in real time and later perform analytics on the same game results in the order they came at the end of business hours. Which of the AWS service will be the best fit for your needs?</p>,multiple-choice,<p>SQS FIFO Queues</p>,<p>SQS Standard Queues</p>,<p>DynamoDB</p>,<p>Kinesis Data Streams</p>,,,4,"Here Kinesis is the best fit as the data can be replayed in the same order. SQS does not allow data replays, and DynamoDB would allow to replay some data, but it’d be different to get some ordering constraints working as well as well as enable real time use cases.",
"<p>Of the 4 four types of identity principals for authentication supported by AWS IoT, which one is commonly used by mobile applications?</p>",multiple-choice,<p>Cognito Identities</p>,"<p>IAM users, groups and roles</p>",<p>X.509 certificates</p>,<p>Federated Identities</p>,,,1,"For IoT mobile applications, the standard is to leverage Cognito.",
"<p>You have programmed a Lambda function that will be automating the creation of an EMR cluster, which in turn should perform some transformations in S3 through EMRFS. Your Lambda function will be triggered by CloudWatch Events. How can you ensure your Lambda function can properly perform its actions?</p>",multiple-choice,<p>Attach an IAM user</p>,<p>Attach an IAM role</p>,<p>Deploy in a VPC</p>,<p>Use a Lambda policy</p>,,,2,"As the Lambda function will be performing API calls on our infrastructure, it needs to have an IAM role attached. Deploying it in a VPC won’t help, and we can’t create users for Lambda functions. ",
"<p>You are working for a data warehouse company that uses Amazon RedShift cluster. For security reasons, it is required that VPC flow logs should be analyzed by Athena to monitor all COPY and UNLOAD traffic of the cluster that moves in and out of the VPC. Which of the following helps you in this regard ?</p>",multiple-choice,<p>Use Redshift Spectrum</p>,<p>Use Redshift WLM</p>,<p>By enabling audit logging in Redshift cluster</p>,<p>Use Enhanced VPC Routing</p>,,,4,"Enhanced VPC Routing forces Redshift to use the VPC for all COPY and UNLOAD commands, which in turn will help us make sure that all that traffic appears on the VPC Flow logs. ",
"<p>You are using an EMR cluster to perform a series of Hive jobs that quickly read and write data to and from S3. When you were running these steps manually through the Hue UI, all worked, but as soon as you automated the steps to run immediately one after the other, the queries were producing results with old data. Upon checking S3, you were able to see the new data was written. What should you do to fix the issue?</p>",multiple-choice,<p>Use Hive consistent write mode to S3</p>,<p>Enable EMRFS</p>,<p>Add an S3 bucket policy to prevent overwrites</p>,<p>Enable S3 versioning</p>,,,2,"S3 has eventually consistent properties, and to fix the problems this might imply in EMR, you need to enable EMRFS. ",
